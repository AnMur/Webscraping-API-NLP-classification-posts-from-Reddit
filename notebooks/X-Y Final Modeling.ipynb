{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a74e4b0",
   "metadata": {},
   "source": [
    "## Notebook 5. Improving Models and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648a4d4",
   "metadata": {},
   "source": [
    "##### For my final modeling:\n",
    "- From my previous notebook, I wanted to explore and work more on the Random Foresrt model using pipelines and GridSearch. Here, I'm goin to see if I could improve the score and see what this model would tell me. I'm also going to remove words 'read' and 'book' from text using the stop words to challange the model and probably reduce the variance.\n",
    "\n",
    "##### For Evaluation:\n",
    "* Instantiate and display the counfusion matrix \n",
    "* Get the true negative, false positive, false negative, and true positive\n",
    "* Create and display predictions for each single post, save predictions as separate csv file\n",
    "\n",
    "##### Make Final Final Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd01a5",
   "metadata": {},
   "source": [
    "### 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928a9047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc1c0ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit_Parenting</th>\n",
       "      <th>subreddit_books</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>why do you like james joyce james joyce from m...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>we yevgeny zamyatin spoilers just finished thi...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  subreddit_Parenting  \\\n",
       "0  why do you like james joyce james joyce from m...                    0   \n",
       "1  we yevgeny zamyatin spoilers just finished thi...                    0   \n",
       "\n",
       "   subreddit_books  \n",
       "0                1  \n",
       "1                1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading dataset\n",
    "\n",
    "df = pd.read_csv('../data/cleaned_df.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce42f80",
   "metadata": {},
   "source": [
    "### 2. Adding 'read' and 'book' to stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ae43e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover stop words\n",
    "%store -r sw\n",
    "\n",
    "np.array(sw)[0:5]\n",
    "\n",
    "sw = sw + ['read', 'book']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0dee6c",
   "metadata": {},
   "source": [
    "### 3. Setting target and train test split my model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff81065",
   "metadata": {},
   "source": [
    "I'm going to again set up the target - subreddit books, predictors - text and perform the train_test_split.\n",
    "\n",
    " - Train-test-split will automatically devide dataset to four parts. Train model on %75 of data and test on %25.\n",
    " - Random_state=42 will ensure that splits model generates are reproducible. \n",
    " - Option stratify will ensure that we have a balance of our target varable in both the training and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57eae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify target and features\n",
    "# Test train split\n",
    "\n",
    "X = df.text\n",
    "y = df.subreddit_books\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42,\n",
    "                                                           stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe4a528",
   "metadata": {},
   "source": [
    "### 4. Instantiating the Pipe(Tf-Idf and Random Forest) and Setting Parameters for the Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b602371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate TfIdf Vectorizer + Random Forest Classifer pipeline\n",
    "\n",
    "pipe = Pipeline([  ('tfidf', TfidfVectorizer()), ('rf', RandomForestClassifier()) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5dd9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary of hyperparameters\n",
    "# My parametrer choice is based on GA lessons recommendations\n",
    "\n",
    "params = {\n",
    "    'tfidf__stop_words': [sw],\n",
    "    'tfidf__max_df' :    [.75, .8, .9, .95],\n",
    "    'tfidf__min_df':    [2, 4, 8],\n",
    "    'tfidf__max_features': [10, 100, 500, 1000, 3000],\n",
    "    'tfidf__norm' : ['l1', 'l2'],\n",
    "    'rf__n_estimators': [10, 50, 100],\n",
    "    'rf__max_depth': [None, 3, 5],\n",
    "    'rf__max_depth': [None, 3, 5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7677cb8",
   "metadata": {},
   "source": [
    "### 5. Fitting the Grid Search and Evaluating scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9b851498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1080 candidates, totalling 5400 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                       ('rf', RandomForestClassifier())]),\n",
       "             param_grid={'rf__max_depth': [None, 3, 5],\n",
       "                         'rf__n_estimators': [10, 50, 100],\n",
       "                         'tfidf__max_df': [0.75, 0.8, 0.9, 0.95],\n",
       "                         'tfidf__max_features': [10, 100, 500, 1000, 3000],\n",
       "                         'tfidf__min_df': [2, 4, 8],\n",
       "                         'tfidf__norm': ['l1', 'l2'],\n",
       "                         'tfidf__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
       "                                                'our', 'ours', 'ourselves',\n",
       "                                                'you', \"you're\", \"you've\",\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...]]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiate GridSearchCV object\n",
    "\n",
    "gs = GridSearchCV(pipe, \n",
    "                  param_grid=params, \n",
    "                  cv=5,\n",
    "                  verbose=1)\n",
    "\n",
    "\n",
    "gs.fit(X_train, y_train) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41947cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs best score: 0.9745415770250663\n",
      "gs best parameters: {'rf__max_depth': None, 'rf__n_estimators': 100, 'tfidf__max_df': 0.75, 'tfidf__max_features': 3000, 'tfidf__min_df': 8, 'tfidf__norm': 'l2', 'tfidf__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'with', 'doing', 'from', 'however', \"we're\", \"you've\", 'these', 'have', \"shan't\", 'but', 'his', 'cannot', 'most', 'any', 'com', 'until', \"i'd\", 'www', 'own', 'also', 'during', \"he'd\", 'about', 'out', 'http', 'this', 'while', \"he'll\", 'they', 'over', 'been', 'be', 'each', 'were', 'myself', 'yourselves', 'both', 'do', 'between', 'since', 'being', \"here's\", 'up', 'when', 'its', 'she', \"hasn't\", 'get', \"wasn't\", 'there', 'if', 'few', 'my', \"there's\", 'why', 'itself', 'further', \"don't\", 'through', \"we'll\", \"doesn't\", 'a', 'hers', 'here', \"you'll\", 'before', 'an', 'those', 'is', 'theirs', \"hadn't\", \"i'll\", 'below', 'and', 'am', 'more', \"they've\", 'to', \"shouldn't\", 'nor', 'does', 'by', 'into', \"mustn't\", 'we', 'under', \"won't\", \"when's\", \"can't\", \"they're\", 'else', 'or', \"what's\", 'how', 'where', 'like', 'are', \"where's\", 'as', 'did', \"we'd\", \"i've\", 'just', \"how's\", 'too', 'no', \"weren't\", 'him', 'me', 'who', 'ever', 'for', 'ought', 'i', 'then', 'he', \"they'd\", 'above', \"isn't\", \"you're\", 'your', 'our', 'r', 'same', 'himself', \"she's\", 'all', 'on', 'again', \"why's\", 'whom', 'so', 'had', \"she'll\", 'ourselves', 'k', 'themselves', 'because', \"let's\", 'ours', 'her', 'was', 'could', 'off', 'than', 'their', \"who's\", 'has', \"didn't\", \"they'll\", 'which', 'shall', 'therefore', 'yours', 'having', 'very', 'down', 'only', \"you'd\", \"that's\", \"we've\", \"wouldn't\", \"he's\", 'once', 'otherwise', 'that', \"she'd\", 'of', 'in', 'such', 'should', 'can', \"couldn't\", \"i'm\", \"haven't\", 'at', 'it', 'not', 'the', 'other', 'hence', 'yourself', 'them', 'herself', 'what', 'against', 'after', \"it's\", 'some', 'you', 'would', \"aren't\", 'time', 'gay', 'know', 'want', 'read', 'book']}\n",
      "gs best estimator: Pipeline(steps=[('tfidf',\n",
      "                 TfidfVectorizer(max_df=0.75, max_features=3000, min_df=8,\n",
      "                                 stop_words=['i', 'me', 'my', 'myself', 'we',\n",
      "                                             'our', 'ours', 'ourselves', 'you',\n",
      "                                             \"you're\", \"you've\", \"you'll\",\n",
      "                                             \"you'd\", 'your', 'yours',\n",
      "                                             'yourself', 'yourselves', 'he',\n",
      "                                             'him', 'his', 'himself', 'she',\n",
      "                                             \"she's\", 'her', 'hers', 'herself',\n",
      "                                             'it', \"it's\", 'its', 'itself', ...])),\n",
      "                ('rf', RandomForestClassifier())])\n"
     ]
    }
   ],
   "source": [
    "print('gs best score:', gs.best_score_)\n",
    "print('gs best parameters:', gs.best_params_)\n",
    "print('gs best estimator:', gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "42a46bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 1.0\n",
      "test score: 0.9743833017077799\n"
     ]
    }
   ],
   "source": [
    "print('train score:', gs.score(X_train,y_train))\n",
    "print('test score:',  gs.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdea28f1",
   "metadata": {},
   "source": [
    "### After exploring the scores it is possible to make next predicitions:\n",
    "\n",
    "- Establishing Pipes and Grid Search definitely helps a lot when working with large sets of data and speeds up the process. Too bad i didn't use it in my previous notebook, but would definitely use it in the future.\n",
    "- The model worked well even I removed 2 most popular words from book dataset\n",
    "- the model chooses the best score, i believe it is a cross val score close to test score.\n",
    "- the train score is still high and if I have more time, I would apply standard scaler to the model to check if it helps.\n",
    "- the grid search picked my lowest max_df and highest max_features. I believe it did to have less variance, because of still high numbers of root words like reading and books.\n",
    "- Number of estimators is 100 what I thought it would take\n",
    "- L2 regularization  were choosen because it tries to estimate the mean of the data to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5bd766",
   "metadata": {},
   "source": [
    "### 6. Predictions and Model Evaluation using Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "33d4bda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rate: 0.9743833017077799\n",
      "Misclassification rate: 0.02561669829222013\n"
     ]
    }
   ],
   "source": [
    "#Predictions\n",
    "# Ideas how to build and plot the confusion matrix came from the General Assembly lessons\n",
    "\n",
    "pred = gs.predict(X_test)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, pred).ravel()\n",
    "\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "misclassification = 1 - accuracy\n",
    "\n",
    "print('Accuracy rate:', accuracy)\n",
    "print('Misclassification rate:', misclassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "26dac1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted words in book</th>\n",
       "      <th>predicted words in parenting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual books</th>\n",
       "      <td>1019</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual parenting</th>\n",
       "      <td>28</td>\n",
       "      <td>1035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  predicted words in book  predicted words in parenting\n",
       "Actual books                         1019                            26\n",
       "Actual parenting                       28                          1035"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting confusion matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, pred)\n",
    "cm_df = pd.DataFrame(data=cm, columns=['predicted words in book', \n",
    "                                       'predicted words in parenting'], \n",
    "                     index=['Actual books', 'Actual parenting'])\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759f887",
   "metadata": {},
   "source": [
    " - From this chart above we can see that model did pretty nice job in predicting posts. It predicted 1019 posts from 1045 that belong to books and 1035 posts from 1063 that belong to parenting.\n",
    " \n",
    " \n",
    "\n",
    "For future plotting it is possible to use this link:\n",
    "https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6cf1f7",
   "metadata": {},
   "source": [
    "### 7. Looking at the posts..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6bd27f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the probabilities\n",
    "\n",
    "probs = gs.predict_proba(X_test)\n",
    "preds = probs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "67c19f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all probablities to a data frame\n",
    "# This idea was found from former GA's student Annet Kerr and\n",
    "# https://stackoverflow.com/questions/25009284/how-to-plot-roc-curve-in-python\n",
    "\n",
    "pred_df = pd.DataFrame({'posts': X_test,\n",
    "                        'y_test_score': y_test,\n",
    "                        'prediction': preds,\n",
    "                        'proba_parenting': probs[:,0],\n",
    "                        'proba_books': probs[:,1]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "24a74d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posts</th>\n",
       "      <th>y_test_score</th>\n",
       "      <th>prediction</th>\n",
       "      <th>proba_parenting</th>\n",
       "      <th>proba_books</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>help book depository keeps refunding i am so s...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5617</th>\n",
       "      <td>feeling guilty getting a kid kicked out of day...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6585</th>\n",
       "      <td>can i live a full life as a parent i need help...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934</th>\n",
       "      <td>oz series guide i want to read all the books i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6035</th>\n",
       "      <td>my f son m has an eating problem and my boyfri...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  posts  y_test_score  \\\n",
       "822   help book depository keeps refunding i am so s...             1   \n",
       "5617  feeling guilty getting a kid kicked out of day...             0   \n",
       "6585  can i live a full life as a parent i need help...             0   \n",
       "2934  oz series guide i want to read all the books i...             1   \n",
       "6035  my f son m has an eating problem and my boyfri...             0   \n",
       "\n",
       "      prediction  proba_parenting  proba_books  \n",
       "822         0.59             0.41         0.59  \n",
       "5617        0.08             0.92         0.08  \n",
       "6585        0.06             0.94         0.06  \n",
       "2934        0.98             0.02         0.98  \n",
       "6035        0.09             0.91         0.09  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2a264465",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df.to_csv('../data/predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ff2131",
   "metadata": {},
   "source": [
    "### 8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9473c",
   "metadata": {},
   "source": [
    "**Super great News!!!! The model sucessfully classifies the posts looking at the words and we could clearly see it from the predicted dataframe. The logistic regression is a winning model, but random forest with some adjustments also predict subreddits very well. The average preddicted percentage is about %97-98.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
